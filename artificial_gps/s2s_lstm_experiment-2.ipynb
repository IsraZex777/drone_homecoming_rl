{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce636f9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:76% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:76% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1aa4bba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from pickle import (\n",
    "    dump,\n",
    "    load\n",
    ")\n",
    "from tensorflow.keras import (\n",
    "    layers,\n",
    "    Model,\n",
    "    Sequential,\n",
    "    optimizers\n",
    ")\n",
    "from pprint import pprint\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79ce498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"./\"\n",
    "DATA_FOLDER_NAME = \"data\"\n",
    "DATA_FOLDER_PATH = os.path.join(BASE_DIR, DATA_FOLDER_NAME)\n",
    "MODELS_FOLDER_NAME = \"models\"\n",
    "MODELS_FOLDER_PATH = os.path.join(BASE_DIR, MODELS_FOLDER_NAME)\n",
    "TUNERS_FOLDER_NAME = \"tuners\"\n",
    "TUNERS_FOLDER_PATH = os.path.join(BASE_DIR, TUNERS_FOLDER_NAME)\n",
    "\n",
    "\n",
    "OUTPUT_DATA_COLUMNS = [\"position_x\", \"position_y\", \"position_z\"]\n",
    "INPUT_DATA_COLUMNS = [\"angular_acceleration_x\", \"angular_acceleration_y\", \"angular_acceleration_z\",\n",
    "                      \"angular_velocity_x\", \"angular_velocity_y\", \"angular_velocity_z\",\n",
    "                      \"linear_acceleration_x\", \"linear_acceleration_y\", \"linear_acceleration_z\",\n",
    "                      \"linear_velocity_x\", \"linear_velocity_y\", \"linear_velocity_z\",\n",
    "                      \"orientation_x\", \"orientation_y\", \"orientation_z\", \"orientation_w\", \"motor_state_timestamp\",\n",
    "                      \"barometer_altitude\", \"barometer_pressure\", \"barometer_qnh\", \"barometer_timestamp\",\n",
    "                      \"magnetometer_magnetic_field_body_x\", \"magnetometer_magnetic_field_body_y\",\n",
    "                      \"magnetometer_magnetic_field_body_x\", \"magnetometer_timestamp\",\n",
    "                      \"rotor_a_speed\", \"rotor_a_thrust\", \"rotor_a_torque_scaler\",\n",
    "                      \"rotor_b_speed\", \"rotor_b_thrust\", \"rotor_b_torque_scaler\",\n",
    "                      \"rotor_c_speed\", \"rotor_c_thrust\", \"rotor_c_torque_scaler\",\n",
    "                      \"rotor_d_speed\", \"rotor_d_thrust\", \"rotor_d_torque_scaler\",\n",
    "                      \"rotor_timestamp\"\n",
    "                     ]\n",
    "\n",
    "TIMESTAMP_COLUMNS = [\n",
    "    \"motor_state_timestamp\",\n",
    "    \"barometer_timestamp\",\n",
    "    \"magnetometer_timestamp\",\n",
    "    \"rotor_timestamp\"\n",
    "]\n",
    "\n",
    "\n",
    "INPUT_SEQUENCE_COLUMNS = [\"angular_acceleration_x\", \"angular_acceleration_y\", \"angular_acceleration_z\",\n",
    "                          \"linear_acceleration_x\", \"linear_acceleration_y\", \"linear_acceleration_z\",\n",
    "                          \"orientation_x\", \"orientation_y\", \"orientation_z\", \"orientation_w\", \"motor_state_timestamp\",\n",
    "                          \"barometer_altitude\", \"barometer_pressure\", \"barometer_qnh\", \"barometer_timestamp\",\n",
    "                          \"magnetometer_magnetic_field_body_x\", \"magnetometer_magnetic_field_body_y\",\n",
    "                          \"magnetometer_magnetic_field_body_x\", \"magnetometer_timestamp\",\n",
    "                          \"rotor_a_speed\", \"rotor_a_thrust\", \"rotor_a_torque_scaler\",\n",
    "                          \"rotor_b_speed\", \"rotor_b_thrust\", \"rotor_b_torque_scaler\",\n",
    "                          \"rotor_c_speed\", \"rotor_c_thrust\", \"rotor_c_torque_scaler\",\n",
    "                          \"rotor_d_speed\", \"rotor_d_thrust\", \"rotor_d_torque_scaler\",\n",
    "                          \"rotor_timestamp\"\n",
    "                         ]\n",
    "OUTPUT_SEQUENCE_COLUMNS = [\"position_x\", \"position_y\", \"position_z\"]\n",
    "MAIN_TIMESTAMP_COLUMN = \"motor_state_timestamp\"\n",
    "INPUT_SEQUENCE_LENGTH = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "460feb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_with_scalers_binary(model, scaler_x, scaler_y, model_name: str):\n",
    "    \"\"\"\n",
    "    Saves models with the x, y scaler objects to a binary library using pickle library\n",
    "    \"\"\"\n",
    "    model_file_name = f\"{model_name}_model.pkl\"\n",
    "    model_file_path = os.path.join(MODELS_FOLDER_PATH, model_file_name)\n",
    "    scaler_x_file_name = f\"{model_name}_scaler_x.pkl\"\n",
    "    scaler_x_file_path = os.path.join(MODELS_FOLDER_PATH, scaler_x_file_name)\n",
    "    scaler_y_file_name = f\"{model_name}_scaler_y.pkl\"\n",
    "    scaler_y_file_path = os.path.join(MODELS_FOLDER_PATH, scaler_y_file_name)\n",
    "\n",
    "    with open(model_file_path, \"wb\") as file:\n",
    "        dump(model, file)\n",
    "\n",
    "    with open(scaler_x_file_path, \"wb\") as file:\n",
    "        dump(scaler_x, file)\n",
    "\n",
    "    with open(scaler_y_file_path, \"wb\") as file:\n",
    "        dump(scaler_y, file)\n",
    "        \n",
    "def load_model_with_scalers_binary(model_name: str):\n",
    "    \"\"\"\n",
    "    Saves models with the x, y scaler objects to a binary library using pickle library\n",
    "    \"\"\"\n",
    "    model_file_name = f\"{model_name}_model.pkl\"\n",
    "    model_file_path = os.path.join(MODELS_FOLDER_PATH, model_file_name)\n",
    "    scaler_x_file_name = f\"{model_name}_scaler_x.pkl\"\n",
    "    scaler_x_file_path = os.path.join(MODELS_FOLDER_PATH, scaler_x_file_name)\n",
    "    scaler_y_file_name = f\"{model_name}_scaler_y.pkl\"\n",
    "    scaler_y_file_path = os.path.join(MODELS_FOLDER_PATH, scaler_y_file_name)\n",
    "\n",
    "    with open(model_file_path, \"rb\") as file:\n",
    "        model = load(file)\n",
    "\n",
    "    with open(scaler_x_file_path, \"rb\") as file:\n",
    "        scaler_x = load(file)\n",
    "\n",
    "    with open(scaler_y_file_path, \"rb\") as file:\n",
    "        scaler_y = load(file)\n",
    "\n",
    "    return model, scaler_x, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd1dcad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_data(data: np.array):\n",
    "    \"\"\"\n",
    "    Splits data into train, dev and test\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_len = len(data)\n",
    "\n",
    "    train, dev, test = np.split(data, [int(.7 * data_len), int(.95 * data_len)])\n",
    "\n",
    "    return train, dev, test\n",
    "\n",
    "\n",
    "def _convert_timestamp_to_interval_seconds(flight_input_df: pd.DataFrame, timestamp_columns: list):\n",
    "    \"\"\"\n",
    "    Converts the timestamp fields into the amount of seconds between each two timestamps\n",
    "\n",
    "    Note: each timestamp represents the amount eof NANO seconds (1,000,000,000 nanoseconds = 1 seconds)\n",
    "    \"\"\"\n",
    "    # Converts the start time to time interval\n",
    "    next_time_df = flight_input_df[timestamp_columns].shift(-1)\n",
    "    time_diff_df = (next_time_df - flight_input_df[timestamp_columns]) / 1_000_000_000\n",
    "    flight_input_df.loc[:, timestamp_columns] = time_diff_df\n",
    "    return flight_input_df\n",
    "\n",
    "\n",
    "def _convert_location_to_step(flight_output_df: pd.DataFrame):\n",
    "    next_coordinates_df = flight_output_df.shift(-1)\n",
    "    coordinate_diff = flight_output_df - next_coordinates_df\n",
    "\n",
    "    return coordinate_diff\n",
    "\n",
    "\n",
    "def load_flight_steps_from_file(csv_name: str, input_columns: list, output_columns: list):\n",
    "    \"\"\"\n",
    "\n",
    "    @param csv_name:\n",
    "    @param input_columns:\n",
    "    @param output_columns:\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    if not csv_name.endswith(\"csv\"):\n",
    "        raise ValueError(f\"File with unsupported extension, expected csv (file: {csv_name})\")\n",
    "\n",
    "    csv_path = os.path.join(DATA_FOLDER_PATH, csv_name)\n",
    "    flight_df = pd.read_csv(csv_path)\n",
    "\n",
    "    x_df = flight_df[input_columns].copy()\n",
    "    timestamp_columns = [column for column in input_columns if column in TIMESTAMP_COLUMNS]\n",
    "    x_df = _convert_timestamp_to_interval_seconds(x_df, timestamp_columns)\n",
    "\n",
    "    y_df = flight_df[output_columns].copy()\n",
    "    y_df = _convert_location_to_step(y_df)\n",
    "\n",
    "    # Drops the last record because the process is based of difference\n",
    "    x_df.drop(x_df.tail(1).index, inplace=True)\n",
    "    y_df.drop(y_df.tail(1).index, inplace=True)\n",
    "\n",
    "    return x_df, y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee5e464e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_preprocessed_flight_sequences(input_columns: list, output_columns: list, sequence_length:int):\n",
    "    \"\"\"\n",
    "    Loads flight steps and orders it to sequences of sequence_length length.\n",
    "    In order to feed it to rnn/lstm s2s model\n",
    "\n",
    "    @param input_columns: The input columns\n",
    "    @param output_columns: The outputs columns\n",
    "    @param sequence_length: Target sequence length\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    all_csv_files = os.listdir(DATA_FOLDER_PATH)\n",
    "\n",
    "    # x, y data from all flight sessions\n",
    "    x_sessions = []\n",
    "    y_sessions = []\n",
    "\n",
    "    # The data feed to the rnn model\n",
    "    sequences_x = []\n",
    "    sequences_y = []\n",
    "\n",
    "    for csv_name in all_csv_files:\n",
    "        try:\n",
    "            x_df, y_df = load_flight_steps_from_file(csv_name, input_columns, output_columns)\n",
    "\n",
    "            x_sessions.append(x_df.to_numpy())\n",
    "            y_sessions.append(y_df.to_numpy())\n",
    "\n",
    "        except ValueError as error:\n",
    "            print(str(error))\n",
    "\n",
    "    all_x_data = np.concatenate(x_sessions)\n",
    "    all_y_data = np.concatenate(y_sessions)\n",
    "\n",
    "    # creating normalizers\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    \n",
    "    scaler_x.fit(all_x_data)\n",
    "    scaler_y.fit(all_y_data)\n",
    "\n",
    "    for session_data_x, session_data_y in zip(x_sessions, y_sessions):\n",
    "        normalized_data_x = scaler_x.transform(session_data_x)\n",
    "        steps_amount = normalized_data_x.shape[0]\n",
    "        normalized_data_y = scaler_y.transform(session_data_y)\n",
    "\n",
    "        # Splits the data into data sequences\n",
    "        for offset in range(steps_amount - sequence_length):\n",
    "            sequences_x.append(normalized_data_x[offset: offset + sequence_length, :])\n",
    "            sequences_y.append(np.add.accumulate(normalized_data_y[offset: offset + sequence_length, :]))\n",
    "        \n",
    "    return sequences_x, sequences_y, scaler_x, scaler_y\n",
    "\n",
    "def load_preprocessed_rnn_dataset(input_columns: list, output_columns: list):\n",
    "    \"\"\"\n",
    "    Loads the whole dataset with preprocessing\n",
    "\n",
    "    @param input_columns: Input columns names\n",
    "    @param output_columns: Output columns names\n",
    "    @param sequence_length: The size of input x sequence\n",
    "    @return: Loaded, preprocessed, shuffled, splitted data set\n",
    "    \"\"\"\n",
    "    flight_data_x, flight_data_y, scaler_x, scaler_y = load_preprocessed_flight_sequences(input_columns, output_columns,\n",
    "                                                                                         INPUT_SEQUENCE_LENGTH)\n",
    "\n",
    "    # flight_data_x, flight_data_y = shuffle_data_set(flight_data_x, flight_data_y)\n",
    "    \n",
    "    train_x, dev_x, test_x = split_data(flight_data_x)\n",
    "    train_y, dev_y, test_y = split_data(flight_data_y)\n",
    "    \n",
    "    return train_x, train_y, dev_x, dev_y, scaler_x, scaler_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d66cc2d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, dev_x, dev_y, scaler_x, scaler_y = \\\n",
    "    load_preprocessed_rnn_dataset(INPUT_SEQUENCE_COLUMNS, OUTPUT_SEQUENCE_COLUMNS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45b413c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model(input_columns_amount: int, output_columns_amount: int, sequence_length: int) -> Model:\n",
    "    \"\"\"\n",
    "    Creates LSTM model\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    input_layer = layers.Input(shape=(None,input_columns_amount))\n",
    "    layer = layers.LSTM(512, return_sequences=True)(input_layer)\n",
    "    output_layer = layers.Dense(output_columns_amount)(layer)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=optimizers.Adam(learning_rate=0.01))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def create_model_dynamic(hp: kt.HyperParameters) -> Model:\n",
    "    \"\"\"\n",
    "    Creates LSTM model using the input hyperparameters chose by keras_tuner\n",
    "    \"\"\"\n",
    "    input_layer = layers.Input(shape=(None, hp.get(\"input_columns_amount\")))\n",
    "    lstm_layer_1_units = hp.Int(\"lstm_layer_1_units\", min_value=8, max_value=256, step=8)\n",
    "    layer = layers.LSTM(lstm_layer_1_units, return_sequences=True)(input_layer)\n",
    "    output_layer = layers.Dense(hp.get(\"output_columns_amount\"))(layer)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=optimizers.Adam(learning_rate=0.001, clipvalue=.5))\n",
    "\n",
    "    return model\n",
    "\n",
    "def _create_regulator(hp: kt.HyperParameters, layer_name: str):\n",
    "    regulator_type = kernels = hp.Choice(f\"{layer_name}_regulator_type\", values=[\"l1\", \"l2\"])\n",
    "    kernels = hp.Choice(\"{layer_name}_regulator_kernel\", values=[0.01,0.001,0.1,0.005,0.05])\n",
    "    if regulator_type == \"l1\":\n",
    "        return tf.keras.regularizers.l1(kernels)\n",
    "    return tf.keras.regularizers.l2(kernels)\n",
    "        \n",
    " \n",
    "def create_model_dynamic(hp: kt.HyperParameters) -> Model:\n",
    "    \"\"\"\n",
    "    Creates LSTM model using the input hyperparameters chose by keras_tuner\n",
    "    \"\"\"\n",
    "\n",
    "    input_layer = layers.Input(shape=(None, hp.get(\"input_columns_amount\")))\n",
    "    \n",
    "    lstm_layer_1_units = hp.Int(\"lstm_layer_units\", min_value=32, max_value=256, step=1)\n",
    "    lstm_props = {}\n",
    "    if hp.Boolean(f\"lstm_regulator\"):\n",
    "        lstm_props[\"kernel_regularizer\"] = _create_regulator(hp, \"lstm\")\n",
    "        \n",
    "    layer = layers.LSTM(lstm_layer_1_units, return_sequences=True, **lstm_props)(input_layer)\n",
    "    if hp.Boolean(f\"lstm_dropout\"):\n",
    "        dropout_rate = hp.Float(f\"lstm_dropout_rate\", min_value=0.0, max_value=0.4, step=0.05)\n",
    "        layer = layers.Dropout(dropout_rate)(layer)\n",
    "    \n",
    "    dense_layers = hp.Int(\"dense_layers_amount\", min_value=0, max_value=2)\n",
    "    dense_activation = hp.Choice(\"dense_activation\", [\"sigmoid\", \"relu\", \"sigmoid\", \"relu\", \"tanh\"])\n",
    "\n",
    "    for layer_index in range(1, dense_layers + 1):\n",
    "        layer_name = f\"dense_{layer_index}\"\n",
    "        units = hp.Int(f\"{layer_name}_units\", min_value=16, max_value=128, step=1)\n",
    "        \n",
    "        dense_props = {}\n",
    "        if hp.Boolean(f\"{layer_name}_regulator\"):\n",
    "            dense_props[\"kernel_regularizer\"] = _create_regulator(hp, layer_name)\n",
    "        \n",
    "        layer = layers.Dense(units, activation=dense_activation, **dense_props)(layer)\n",
    "\n",
    "        if hp.Boolean(f\"dense_{layer_index}_dropout\"):\n",
    "            dropout_rate = hp.Float(f\"dense_{layer_index}_dropout_rate\", min_value=0, max_value=0.4, step=0.05)\n",
    "            layer = layers.Dropout(dropout_rate)(layer)\n",
    "\n",
    "    output_layer = layers.Dense(hp.get(\"output_columns_amount\"))(layer)\n",
    "\n",
    "    learning_rate = hp.Choice(\"adam_learning_rate\", [0.1, 0.2, 0.05, 1e-2,2e-2, 1e-3,2e-3,3e-3, 5e-3, 5e-4])\n",
    "    if hp.Boolean(f\"learning_rate_decay\"):\n",
    "        learning_rate = optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=learning_rate,\n",
    "            decay_steps=10000,\n",
    "            decay_rate=0.9)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=optimizers.Adam(learning_rate=learning_rate))\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcf94227",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FORCE_CPU_RUN = False\n",
    "if FORCE_CPU_RUN:\n",
    "    print(\"###########################\")\n",
    "    tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b347ad34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 00:57:31.430484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-07 00:57:31.435341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-07 00:57:31.435862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-07 00:57:31.436962: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-07 00:57:31.455855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-07 00:57:31.456518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-07 00:57:31.457206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-07 00:57:31.695851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-07 00:57:31.696325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-07 00:57:31.696734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-07 00:57:31.697134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9651 MB memory:  -> device: 0, name: GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = create_model(len(INPUT_SEQUENCE_COLUMNS), len(OUTPUT_SEQUENCE_COLUMNS), INPUT_SEQUENCE_LENGTH)\n",
    "\n",
    "# history = model.fit(train_x,\n",
    "#           train_y,\n",
    "#           epochs=100,\n",
    "#           batch_size=256,\n",
    "#           validation_data=(dev_x, dev_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74ca995c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(all_train_loss)\n",
    "# plt.plot(all_dev_loss) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18fc0659",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(all_train_loss[2:])\n",
    "# plt.plot(all_dev_loss[2:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f392ced",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save_model_with_scalers_binary(model, scaler_x, scaler_y, \"s2s_model_static\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27fead02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project /home/israzex/Desktop/drone_homecoming_rl/artificial_gps/tuners/s2s_exp1_6Jan22/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from /home/israzex/Desktop/drone_homecoming_rl/artificial_gps/tuners/s2s_exp1_6Jan22/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "from settings import TUNERS_FOLDER_PATH\n",
    "import copy \n",
    "\n",
    "\n",
    "hp = kt.HyperParameters()\n",
    "hp.Fixed(\"input_columns_amount\", len(INPUT_SEQUENCE_COLUMNS))\n",
    "hp.Fixed(\"output_columns_amount\", len(OUTPUT_SEQUENCE_COLUMNS))\n",
    "\n",
    "project_name = \"s2s_exp1_6Jan22\"\n",
    "tuner = kt.BayesianOptimization(\n",
    "    create_model_dynamic,\n",
    "    hyperparameters=hp,\n",
    "    tune_new_entries=True,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=1000,\n",
    "    directory=TUNERS_FOLDER_PATH,\n",
    "    project_name=project_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a30887b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 00:57:34.672443: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://03dc120a-4081-4cab-97bd-3e2af19e796b/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://03dc120a-4081-4cab-97bd-3e2af19e796b/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fe060f3bc10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "model = tuner.get_best_models(num_models=10)[0]\n",
    "save_model_with_scalers_binary(model, scaler_x, scaler_y, \"s2s_100seq_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd0f56d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 48 Complete [00h 10m 45s]\n",
      "val_loss: 12.985352516174316\n",
      "\n",
      "Best val_loss So Far: 8.593206405639648\n",
      "Total elapsed time: 00h 10m 45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 01:08:47.528086: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0xe6f7310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #49\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "input_columns_a...|32                |32                \n",
      "output_columns_...|3                 |3                 \n",
      "lstm_layer_units  |256               |256               \n",
      "lstm_regulator    |False             |False             \n",
      "lstm_dropout      |False             |True              \n",
      "dense_layers_am...|0                 |0                 \n",
      "dense_activation  |tanh              |tanh              \n",
      "adam_learning_rate|0.0005            |0.0005            \n",
      "learning_rate_d...|False             |True              \n",
      "lstm_regulator_...|l1                |l2                \n",
      "{layer_name}_re...|0.05              |0.05              \n",
      "lstm_dropout_rate |0.4               |0.4               \n",
      "dense_1_units     |128               |128               \n",
      "dense_1_regulator |False             |False             \n",
      "dense_1_dropout   |True              |True              \n",
      "dense_2_units     |16                |16                \n",
      "dense_2_regulator |True              |True              \n",
      "dense_2_dropout   |True              |False             \n",
      "dense_1_regulat...|l2                |l2                \n",
      "dense_1_dropout...|0                 |0.35              \n",
      "dense_2_regulat...|l1                |l1                \n",
      "dense_2_dropout...|0.4               |0                 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 01:08:50.759697: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4029849600 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "  5/615 [..............................] - ETA: 16s - loss: 1027.8944WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0128s vs `on_train_batch_end` time: 0.0178s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0128s vs `on_train_batch_end` time: 0.0178s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615/615 [==============================] - 20s 32ms/step - loss: 163.2887 - val_loss: 55.0815\n",
      "Epoch 2/1000\n",
      "615/615 [==============================] - 18s 30ms/step - loss: 16.9773 - val_loss: 48.0848\n",
      "Epoch 3/1000\n",
      "615/615 [==============================] - 19s 30ms/step - loss: 14.8995 - val_loss: 46.8833\n",
      "Epoch 4/1000\n",
      "615/615 [==============================] - 19s 30ms/step - loss: 14.3027 - val_loss: 48.5078\n",
      "Epoch 5/1000\n",
      "137/615 [=====>........................] - ETA: 12s - loss: 14.2968"
     ]
    }
   ],
   "source": [
    "tensorboard_dir = os.path.join(os.path.join(TUNERS_FOLDER_PATH,project_name), \"tensorboard\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_dir, \n",
    "                                                      histogram_freq=1)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, baseline=80)\n",
    "\n",
    "tuner.search(train_x,\n",
    "            train_y,\n",
    "             validation_data=(dev_x, dev_y),\n",
    "             epochs = 1000,\n",
    "             batch_size=512,\n",
    "             callbacks=[early_stop, tensorboard_callback])\n",
    "\n",
    "# tuner.search(train_x,\n",
    "#             train_y,\n",
    "#              validation_data=(dev_x, dev_y),\n",
    "#              epochs = 1000,\n",
    "#              batch_size=512,\n",
    "#              callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac0b53f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# os.environ['TENSORBOARD_BINARY'] = '/home/israzex/Desktop/drone_homecoming_rl/venv/bin/tensorboard'\n",
    "# %tensorboard --logdir \"/home/israzex/Desktop/drone_homecoming_rl/artificial_gps/tuners/s2s_exp1_3/tensorboard\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26246eeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_lstm_model_predictions(model_name: str,\n",
    "                                data_csv_name: str,\n",
    "                                input_columns: list,\n",
    "                                output_columns: list,\n",
    "                                sequence_length: int):\n",
    "    flight_x_df, flight_y_df = load_flight_steps_from_file(data_csv_name, input_columns, output_columns)\n",
    "    data_x = flight_x_df.to_numpy()\n",
    "    real_y = flight_y_df.to_numpy()\n",
    "\n",
    "    try:\n",
    "        model, scaler_x, scaler_y = load_model_with_scalers_binary(model_name)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"There is no model in name: {model_name}\")\n",
    "        return\n",
    "\n",
    "    normalized_real_x = scaler_x.transform(data_x)\n",
    "    recording_length = data_x.shape[0]\n",
    "    \n",
    "    \n",
    "\n",
    "    # Splits the data into data sequences\n",
    "    sequences_x = []\n",
    "    for offset in range(recording_length - sequence_length):\n",
    "        sequences_x.append(normalized_real_x[offset: offset + sequence_length, :])\n",
    "\n",
    "    sequences_x = np.stack(sequences_x)\n",
    "    predicted_sequence = model.predict(sequences_x)\n",
    "    predicted_sequence_len = predicted_sequence.shape[0]\n",
    "    \n",
    "    \n",
    "    # opposite operation of np.add.accumulate\n",
    "    for pred_index in range(predicted_sequence_len):\n",
    "        for seq_index in range(sequence_length - 1, 0 ,-1) :\n",
    "            predicted_sequence[pred_index][seq_index] -= predicted_sequence[pred_index][seq_index - 1]\n",
    "    \n",
    "    predicted_values = np.zeros(real_y.shape)\n",
    "\n",
    "    # finds best betta\n",
    "    betta = 0.0\n",
    "    best_betta = 0.0\n",
    "    normalized_real_y = scaler_y.transform(real_y)\n",
    "    lowerst_mse = sys.float_info.max\n",
    "    while betta < 1:\n",
    "        predicted_values = np.zeros(real_y.shape)\n",
    "        values_in_average = np.zeros(real_y.shape)\n",
    "        for seq_index in range(predicted_sequence_len):\n",
    "            values_in_average[seq_index:seq_index + sequence_length] += 1\n",
    "            predicted_values[seq_index:seq_index + sequence_length] = \\\n",
    "                (betta * predicted_values[seq_index:seq_index + sequence_length] + \n",
    "                (1 - betta) * predicted_sequence[seq_index, :]) \n",
    "#             / \\\n",
    "#             (1 - np.power(betta, values_in_average[seq_index:seq_index + sequence_length]))\n",
    "\n",
    "        mse = ((predicted_values - normalized_real_y)**2).mean(axis=0).reshape((3,1))\n",
    "        if not np.isnan(np.sum(mse)) and np.sum(np.sqrt(np.power(mse, 2))) < np.sum(np.sqrt(np.power(lowerst_mse,2))):\n",
    "#             if not np.isnan(np.sum(mse)) and np.sum(mse) < np.sum(lowerst_mse):\n",
    "            lowerst_mse = mse\n",
    "            best_betta = betta\n",
    "            print(mse)\n",
    "        print(betta)\n",
    "        betta += 0.01\n",
    "        \n",
    "    # Inserts all the predicted values using Exponentially Weighted Averages with bias correction\n",
    "    values_in_average = np.zeros(real_y.shape)\n",
    "#     betta = 0.94\n",
    "    betta = best_betta\n",
    "    for seq_index in range(predicted_sequence_len):\n",
    "        values_in_average[seq_index:seq_index + sequence_length] += 1\n",
    "        predicted_values[seq_index:seq_index + sequence_length] = \\\n",
    "            (betta * predicted_values[seq_index:seq_index + sequence_length] + \n",
    "            (1 - betta) * predicted_sequence[seq_index, :]) \n",
    "    \n",
    "    \n",
    "#     normalized_real_y = scaler_y.transform(real_y)\n",
    "#     print(predicted_values[100:110])\n",
    "#     print(normalized_real_y[100:110])\n",
    "\n",
    "#     # Inserts all the predicted values using Exponentially Weighted Averages with bias correction\n",
    "#     values_in_average = np.zeros(real_y.shape)\n",
    "#     for seq_index in range(predicted_sequence_len):\n",
    "#         values_in_average[seq_index:seq_index + sequence_length] += 1\n",
    "#         predicted_values[seq_index:seq_index + sequence_length] += predicted_sequence[seq_index,:]\n",
    "#     predicted_values = predicted_values /  values_in_average \n",
    "    \n",
    "    predicted_values = scaler_y.inverse_transform(predicted_values)\n",
    "    \n",
    "    print(real_y[100:110])\n",
    "    print(predicted_values[100:110])\n",
    "    \n",
    "    predicted_offset = np.add.accumulate(predicted_values)\n",
    "    real_offset = np.add.accumulate(real_y)\n",
    "    \n",
    "    print(predicted_offset[100:120])\n",
    "    print(real_offset[100:120])\n",
    "        \n",
    "#     print(real_offset)\n",
    "    time_intervals = flight_x_df[MAIN_TIMESTAMP_COLUMN].to_numpy().reshape(-1, 1)\n",
    "    time_offset = np.add.accumulate(time_intervals)\n",
    "    \n",
    "    return predicted_offset, real_offset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee08bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = tuner.get_best_models(num_models=10)[0]\n",
    "# save_model_with_scalers_binary(model, scaler_x, scaler_y, \"s2s_model_4\")\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359edaaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trained_on = [\"flight_2021:12:26_21:05:34_1_record.csv\",\n",
    "             \"create_10_seconds_down_only_flight_record_data.csv\",\n",
    "              \"_flight_2021:12:31_22:00:06_record.csv\"]\n",
    "new_data = [\"flight_2021:12:31_22:03:16_record.csv\",\n",
    "            \"flight_2021:12:28_00:25:05_1_record.csv\"]\n",
    "\n",
    "predicted_offset, real_offset = test_lstm_model_predictions(\"s2s_50seq_1\",\n",
    "                                                                             trained_on[0],\n",
    "                                                                             INPUT_SEQUENCE_COLUMNS,\n",
    "                                                                             OUTPUT_SEQUENCE_COLUMNS,\n",
    "                                                                             INPUT_SEQUENCE_LENGTH)\n",
    "# predicted_offset, real_offset, time_offset  = test_model_predictions(\"s2s_model_static\",\n",
    "#                        trained_on[0],\n",
    "#                        INPUT_SEQUENCE_COLUMNS,\n",
    "#                        OUTPUT_SEQUENCE_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326f1c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_real_y[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8148c119",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38298504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(predicted_offset[:,0])\n",
    "plt.plot(real_offset[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ce6702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(predicted_offset[:,1])\n",
    "plt.plot(real_offset[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862e6306",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(predicted_offset[:,2])\n",
    "plt.plot(real_offset[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd78f3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5599772",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef5c7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"s2s_50seq_1\"\n",
    "data_csv_name = trained_on[0],\n",
    "input_columns = INPUT_SEQUENCE_COLUMNS,\n",
    "output_columns = OUTPUT_SEQUENCE_COLUMNS,\n",
    "sequence_length = INPUT_SEQUENCE_LENGTH\n",
    "\n",
    "flight_x_df, flight_y_df = load_flight_steps_from_file(data_csv_name, input_columns, output_columns)\n",
    "data_x = flight_x_df.to_numpy()\n",
    "real_y = flight_y_df.to_numpy()\n",
    "\n",
    "try:\n",
    "    model, scaler_x, scaler_y = load_model_with_scalers_binary(model_name)\n",
    "except FileNotFoundError:\n",
    "    print(f\"There is no model in name: {model_name}\")\n",
    "    return\n",
    "\n",
    "normalized_real_x = scaler_x.transform(data_x)\n",
    "recording_length = data_x.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "# Splits the data into data sequences\n",
    "sequences_x = []\n",
    "for offset in range(recording_length - sequence_length):\n",
    "    sequences_x.append(normalized_real_x[offset: offset + sequence_length, :])\n",
    "\n",
    "sequences_x = np.stack(sequences_x)\n",
    "predicted_sequence = model.predict(sequences_x)\n",
    "predicted_sequence_len = predicted_sequence.shape[0]\n",
    "\n",
    "\n",
    "# opposite operation of np.add.accumulate\n",
    "for pred_index in range(predicted_sequence_len):\n",
    "    for seq_index in range(sequence_length - 1, 0 ,-1) :\n",
    "        predicted_sequence[pred_index][seq_index] -= predicted_sequence[pred_index][seq_index - 1]\n",
    "\n",
    "predicted_values = np.zeros(real_y.shape)\n",
    "\n",
    "# finds best betta\n",
    "betta = 0.0\n",
    "best_betta = 0.0\n",
    "normalized_real_y = scaler_y.transform(real_y)\n",
    "lowerst_mse = sys.float_info.max\n",
    "while betta < 1:\n",
    "    predicted_values = np.zeros(real_y.shape)\n",
    "    values_in_average = np.zeros(real_y.shape)\n",
    "    for seq_index in range(predicted_sequence_len):\n",
    "        values_in_average[seq_index:seq_index + sequence_length] += 1\n",
    "        predicted_values[seq_index:seq_index + sequence_length] = \\\n",
    "            (betta * predicted_values[seq_index:seq_index + sequence_length] + \n",
    "            (1 - betta) * predicted_sequence[seq_index, :]) \n",
    "#             / \\\n",
    "#             (1 - np.power(betta, values_in_average[seq_index:seq_index + sequence_length]))\n",
    "\n",
    "    mse = ((predicted_values - normalized_real_y)**2).mean(axis=0).reshape((3,1))\n",
    "    if not np.isnan(np.sum(mse)) and np.sum(np.sqrt(np.power(mse, 2))) < np.sum(np.sqrt(np.power(lowerst_mse,2))):\n",
    "#             if not np.isnan(np.sum(mse)) and np.sum(mse) < np.sum(lowerst_mse):\n",
    "        lowerst_mse = mse\n",
    "        best_betta = betta\n",
    "        print(mse)\n",
    "    print(betta)\n",
    "    betta += 0.01\n",
    "\n",
    "# Inserts all the predicted values using Exponentially Weighted Averages with bias correction\n",
    "values_in_average = np.zeros(real_y.shape)\n",
    "#     betta = 0.94\n",
    "betta = best_betta\n",
    "for seq_index in range(predicted_sequence_len):\n",
    "    values_in_average[seq_index:seq_index + sequence_length] += 1\n",
    "    predicted_values[seq_index:seq_index + sequence_length] = \\\n",
    "        (betta * predicted_values[seq_index:seq_index + sequence_length] + \n",
    "        (1 - betta) * predicted_sequence[seq_index, :]) \n",
    "\n",
    "\n",
    "#     normalized_real_y = scaler_y.transform(real_y)\n",
    "#     print(predicted_values[100:110])\n",
    "#     print(normalized_real_y[100:110])\n",
    "\n",
    "#     # Inserts all the predicted values using Exponentially Weighted Averages with bias correction\n",
    "#     values_in_average = np.zeros(real_y.shape)\n",
    "#     for seq_index in range(predicted_sequence_len):\n",
    "#         values_in_average[seq_index:seq_index + sequence_length] += 1\n",
    "#         predicted_values[seq_index:seq_index + sequence_length] += predicted_sequence[seq_index,:]\n",
    "#     predicted_values = predicted_values /  values_in_average \n",
    "\n",
    "predicted_values = scaler_y.inverse_transform(predicted_values)\n",
    "\n",
    "print(real_y[100:110])\n",
    "print(predicted_values[100:110])\n",
    "\n",
    "predicted_offset = np.add.accumulate(predicted_values)\n",
    "real_offset = np.add.accumulate(real_y)\n",
    "\n",
    "print(predicted_offset[100:120])\n",
    "print(real_offset[100:120])\n",
    "\n",
    "#     print(real_offset)\n",
    "time_intervals = flight_x_df[MAIN_TIMESTAMP_COLUMN].to_numpy().reshape(-1, 1)\n",
    "time_offset = np.add.accumulate(time_intervals)\n",
    "\n",
    "return predicted_offset, real_offset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33595078",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a39c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca535de9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdf8b09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ef11ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb9ec02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e6744e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35044b0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drone_venv",
   "language": "python",
   "name": "drone_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
