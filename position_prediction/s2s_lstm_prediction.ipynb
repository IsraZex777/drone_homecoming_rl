{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0386ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "021edb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from pickle import (\n",
    "    dump,\n",
    "    load\n",
    ")\n",
    "from tensorflow.keras import (\n",
    "    layers,\n",
    "    Model,\n",
    "    Sequential,\n",
    "    optimizers\n",
    ")\n",
    "from pprint import pprint\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "074b9c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"./\"\n",
    "DATA_FOLDER_NAME = \"data\"\n",
    "DATA_FOLDER_PATH = os.path.join(BASE_DIR, DATA_FOLDER_NAME)\n",
    "MODELS_FOLDER_NAME = \"models\"\n",
    "MODELS_FOLDER_PATH = os.path.join(BASE_DIR, MODELS_FOLDER_NAME)\n",
    "TUNERS_FOLDER_NAME = \"tuners\"\n",
    "TUNERS_FOLDER_PATH = os.path.join(BASE_DIR, TUNERS_FOLDER_NAME)\n",
    "\n",
    "\n",
    "OUTPUT_DATA_COLUMNS = [\"position_x\", \"position_y\", \"position_z\"]\n",
    "INPUT_DATA_COLUMNS = [\"angular_acceleration_x\", \"angular_acceleration_y\", \"angular_acceleration_z\",\n",
    "                      \"angular_velocity_x\", \"angular_velocity_y\", \"angular_velocity_z\",\n",
    "                      \"linear_acceleration_x\", \"linear_acceleration_y\", \"linear_acceleration_z\",\n",
    "                      \"linear_velocity_x\", \"linear_velocity_y\", \"linear_velocity_z\",\n",
    "                      \"orientation_x\", \"orientation_y\", \"orientation_z\", \"orientation_w\", \"motor_state_timestamp\",\n",
    "                      \"barometer_altitude\", \"barometer_pressure\", \"barometer_qnh\", \"barometer_timestamp\",\n",
    "                      \"magnetometer_magnetic_field_body_x\", \"magnetometer_magnetic_field_body_y\",\n",
    "                      \"magnetometer_magnetic_field_body_x\", \"magnetometer_timestamp\",\n",
    "                      \"rotor_a_speed\", \"rotor_a_thrust\", \"rotor_a_torque_scaler\",\n",
    "                      \"rotor_b_speed\", \"rotor_b_thrust\", \"rotor_b_torque_scaler\",\n",
    "                      \"rotor_c_speed\", \"rotor_c_thrust\", \"rotor_c_torque_scaler\",\n",
    "                      \"rotor_d_speed\", \"rotor_d_thrust\", \"rotor_d_torque_scaler\",\n",
    "                      \"rotor_timestamp\"\n",
    "                     ]\n",
    "\n",
    "TIMESTAMP_COLUMNS = [\n",
    "    \"motor_state_timestamp\",\n",
    "    \"barometer_timestamp\",\n",
    "    \"magnetometer_timestamp\",\n",
    "    \"rotor_timestamp\"\n",
    "]\n",
    "\n",
    "\n",
    "INPUT_SEQUENCE_COLUMNS = [\"angular_acceleration_x\", \"angular_acceleration_y\", \"angular_acceleration_z\",\n",
    "                          \"linear_acceleration_x\", \"linear_acceleration_y\", \"linear_acceleration_z\",\n",
    "                          \"orientation_x\", \"orientation_y\", \"orientation_z\", \"orientation_w\", \"motor_state_timestamp\",\n",
    "                          \"barometer_altitude\", \"barometer_pressure\", \"barometer_qnh\", \"barometer_timestamp\",\n",
    "                          \"magnetometer_magnetic_field_body_x\", \"magnetometer_magnetic_field_body_y\",\n",
    "                          \"magnetometer_magnetic_field_body_x\", \"magnetometer_timestamp\",\n",
    "                          \"rotor_a_speed\", \"rotor_a_thrust\", \"rotor_a_torque_scaler\",\n",
    "                          \"rotor_b_speed\", \"rotor_b_thrust\", \"rotor_b_torque_scaler\",\n",
    "                          \"rotor_c_speed\", \"rotor_c_thrust\", \"rotor_c_torque_scaler\",\n",
    "                          \"rotor_d_speed\", \"rotor_d_thrust\", \"rotor_d_torque_scaler\",\n",
    "                          \"rotor_timestamp\"\n",
    "                         ]\n",
    "OUTPUT_SEQUENCE_COLUMNS = [\"position_x\", \"position_y\", \"position_z\"]\n",
    "MAIN_TIMESTAMP_COLUMN = \"motor_state_timestamp\"\n",
    "INPUT_SEQUENCE_LENGTH = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dee75384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_with_scalers_binary(model, scaler_x, scaler_y, model_name: str):\n",
    "    \"\"\"\n",
    "    Saves models with the x, y scaler objects to a binary library using pickle library\n",
    "    \"\"\"\n",
    "    model_file_name = f\"{model_name}_model.pkl\"\n",
    "    model_file_path = os.path.join(MODELS_FOLDER_PATH, model_file_name)\n",
    "    scaler_x_file_name = f\"{model_name}_scaler_x.pkl\"\n",
    "    scaler_x_file_path = os.path.join(MODELS_FOLDER_PATH, scaler_x_file_name)\n",
    "    scaler_y_file_name = f\"{model_name}_scaler_y.pkl\"\n",
    "    scaler_y_file_path = os.path.join(MODELS_FOLDER_PATH, scaler_y_file_name)\n",
    "\n",
    "    with open(model_file_path, \"wb\") as file:\n",
    "        dump(model, file)\n",
    "\n",
    "    with open(scaler_x_file_path, \"wb\") as file:\n",
    "        dump(scaler_x, file)\n",
    "\n",
    "    with open(scaler_y_file_path, \"wb\") as file:\n",
    "        dump(scaler_y, file)\n",
    "        \n",
    "def load_model_with_scalers_binary(model_name: str):\n",
    "    \"\"\"\n",
    "    Saves models with the x, y scaler objects to a binary library using pickle library\n",
    "    \"\"\"\n",
    "    model_file_name = f\"{model_name}_model.pkl\"\n",
    "    model_file_path = os.path.join(MODELS_FOLDER_PATH, model_file_name)\n",
    "    scaler_x_file_name = f\"{model_name}_scaler_x.pkl\"\n",
    "    scaler_x_file_path = os.path.join(MODELS_FOLDER_PATH, scaler_x_file_name)\n",
    "    scaler_y_file_name = f\"{model_name}_scaler_y.pkl\"\n",
    "    scaler_y_file_path = os.path.join(MODELS_FOLDER_PATH, scaler_y_file_name)\n",
    "\n",
    "    with open(model_file_path, \"rb\") as file:\n",
    "        model = load(file)\n",
    "\n",
    "    with open(scaler_x_file_path, \"rb\") as file:\n",
    "        scaler_x = load(file)\n",
    "\n",
    "    with open(scaler_y_file_path, \"rb\") as file:\n",
    "        scaler_y = load(file)\n",
    "\n",
    "    return model, scaler_x, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f735763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data: np.array):\n",
    "    \"\"\"\n",
    "    Splits data into train, dev and test\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_len = len(data)\n",
    "\n",
    "    train, dev, test = np.split(data, [int(.7 * data_len), int(.95 * data_len)])\n",
    "\n",
    "    return train, dev, test\n",
    "\n",
    "\n",
    "def _convert_timestamp_to_interval_seconds(flight_input_df: pd.DataFrame, timestamp_columns: list):\n",
    "    \"\"\"\n",
    "    Converts the timestamp fields into the amount of seconds between each two timestamps\n",
    "\n",
    "    Note: each timestamp represents the amount eof NANO seconds (1,000,000,000 nanoseconds = 1 seconds)\n",
    "    \"\"\"\n",
    "    # Converts the start time to time interval\n",
    "    next_time_df = flight_input_df[timestamp_columns].shift(-1)\n",
    "    time_diff_df = (next_time_df - flight_input_df[timestamp_columns]) / 1_000_000_000\n",
    "    flight_input_df.loc[:, timestamp_columns] = time_diff_df\n",
    "    return flight_input_df\n",
    "\n",
    "\n",
    "def _convert_location_to_step(flight_output_df: pd.DataFrame):\n",
    "    next_coordinates_df = flight_output_df.shift(-1)\n",
    "    coordinate_diff = flight_output_df - next_coordinates_df\n",
    "\n",
    "    return coordinate_diff\n",
    "\n",
    "\n",
    "def load_flight_steps_from_file(csv_name: str, input_columns: list, output_columns: list):\n",
    "    \"\"\"\n",
    "\n",
    "    @param csv_name:\n",
    "    @param input_columns:\n",
    "    @param output_columns:\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    if not csv_name.endswith(\"csv\"):\n",
    "        raise ValueError(f\"File with unsupported extension, expected csv (file: {csv_name})\")\n",
    "\n",
    "    csv_path = os.path.join(DATA_FOLDER_PATH, csv_name)\n",
    "    flight_df = pd.read_csv(csv_path)\n",
    "\n",
    "    x_df = flight_df[input_columns].copy()\n",
    "    timestamp_columns = [column for column in input_columns if column in TIMESTAMP_COLUMNS]\n",
    "    x_df = _convert_timestamp_to_interval_seconds(x_df, timestamp_columns)\n",
    "\n",
    "    y_df = flight_df[output_columns].copy()\n",
    "    y_df = _convert_location_to_step(y_df)\n",
    "\n",
    "    # Drops the last record because the process is based of difference\n",
    "    x_df.drop(x_df.tail(1).index, inplace=True)\n",
    "    y_df.drop(y_df.tail(1).index, inplace=True)\n",
    "\n",
    "    return x_df, y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97820ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"s2s_50seq_1\"\n",
    "data_csv_name = \"_flight_2021:12:31_21:56:52_record.csv\"\n",
    "input_columns = INPUT_SEQUENCE_COLUMNS\n",
    "output_columns = OUTPUT_SEQUENCE_COLUMNS\n",
    "sequence_length = INPUT_SEQUENCE_LENGTH\n",
    "\n",
    "flight_x_df, flight_y_df = load_flight_steps_from_file(data_csv_name, input_columns, output_columns)\n",
    "data_x = flight_x_df.to_numpy()\n",
    "real_y = flight_y_df.to_numpy()\n",
    "\n",
    "try:\n",
    "    model, scaler_x, scaler_y = load_model_with_scalers_binary(model_name)\n",
    "except FileNotFoundError:\n",
    "    print(f\"There is no model in name: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be09152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_real_x = scaler_x.transform(data_x)\n",
    "recording_length = data_x.shape[0]\n",
    "\n",
    "# Splits the data into data sequences\n",
    "sequences_x = []\n",
    "for offset in range(recording_length - sequence_length):\n",
    "    sequences_x.append(normalized_real_x[offset: offset + sequence_length, :])\n",
    "\n",
    "sequences_x = np.stack(sequences_x)\n",
    "predicted_sequence = model.predict(sequences_x)\n",
    "predicted_sequence_len = predicted_sequence.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8428809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper([None, None])\n",
      "False\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# dir(model)\n",
    "lstm_layer = model.layers[1]\n",
    "print(lstm_layer.states)\n",
    "print(lstm_layer.return_state)\n",
    "print(lstm_layer.units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c51957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# opposite operation of np.add.accumulate\n",
    "for pred_index in range(predicted_sequence_len):\n",
    "    for seq_index in range(sequence_length - 1, 0 ,-1) :\n",
    "        predicted_sequence[pred_index][seq_index] -= predicted_sequence[pred_index][seq_index - 1]\n",
    "\n",
    "predicted_values = np.zeros(real_y.shape)\n",
    "\n",
    "# finds best betta\n",
    "betta = 0.0\n",
    "best_betta = 0.0\n",
    "normalized_real_y = scaler_y.transform(real_y)\n",
    "lowerst_mse = sys.float_info.max\n",
    "while betta < 1:\n",
    "    predicted_values = np.zeros(real_y.shape)\n",
    "    values_in_average = np.zeros(real_y.shape)\n",
    "    for seq_index in range(predicted_sequence_len):\n",
    "        values_in_average[seq_index:seq_index + sequence_length] += 1\n",
    "        predicted_values[seq_index:seq_index + sequence_length] = \\\n",
    "            (betta * predicted_values[seq_index:seq_index + sequence_length] + \n",
    "            (1 - betta) * predicted_sequence[seq_index, :]) \n",
    "#             / \\\n",
    "#             (1 - np.power(betta, values_in_average[seq_index:seq_index + sequence_length]))\n",
    "\n",
    "    mse = ((predicted_values - normalized_real_y)**2).mean(axis=0).reshape((3,1))\n",
    "    if not np.isnan(np.sum(mse)) and np.sum(np.sqrt(np.power(mse, 2))) < np.sum(np.sqrt(np.power(lowerst_mse,2))):\n",
    "#             if not np.isnan(np.sum(mse)) and np.sum(mse) < np.sum(lowerst_mse):\n",
    "        lowerst_mse = mse\n",
    "        best_betta = betta\n",
    "        print(mse)\n",
    "    print(betta)\n",
    "    betta += 0.01\n",
    "\n",
    "# Inserts all the predicted values using Exponentially Weighted Averages with bias correction\n",
    "values_in_average = np.zeros(real_y.shape)\n",
    "#     betta = 0.94\n",
    "betta = best_betta\n",
    "for seq_index in range(predicted_sequence_len):\n",
    "    values_in_average[seq_index:seq_index + sequence_length] += 1\n",
    "    predicted_values[seq_index:seq_index + sequence_length] = \\\n",
    "        (betta * predicted_values[seq_index:seq_index + sequence_length] + \n",
    "        (1 - betta) * predicted_sequence[seq_index, :]) \n",
    "\n",
    "\n",
    "#     normalized_real_y = scaler_y.transform(real_y)\n",
    "#     print(predicted_values[100:110])\n",
    "#     print(normalized_real_y[100:110])\n",
    "\n",
    "#     # Inserts all the predicted values using Exponentially Weighted Averages with bias correction\n",
    "#     values_in_average = np.zeros(real_y.shape)\n",
    "#     for seq_index in range(predicted_sequence_len):\n",
    "#         values_in_average[seq_index:seq_index + sequence_length] += 1\n",
    "#         predicted_values[seq_index:seq_index + sequence_length] += predicted_sequence[seq_index,:]\n",
    "#     predicted_values = predicted_values /  values_in_average \n",
    "\n",
    "predicted_values = scaler_y.inverse_transform(predicted_values)\n",
    "\n",
    "print(real_y[100:110])\n",
    "print(predicted_values[100:110])\n",
    "\n",
    "predicted_offset = np.add.accumulate(predicted_values)\n",
    "real_offset = np.add.accumulate(real_y)\n",
    "\n",
    "print(predicted_offset[100:120])\n",
    "print(real_offset[100:120])\n",
    "\n",
    "#     print(real_offset)\n",
    "time_intervals = flight_x_df[MAIN_TIMESTAMP_COLUMN].to_numpy().reshape(-1, 1)\n",
    "time_offset = np.add.accumulate(time_intervals)\n",
    "\n",
    "return predicted_offset, real_offset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drone_venv",
   "language": "python",
   "name": "drone_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
